# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО                | Роль в проекте              | Оценка       |
|--------------------|-----------------------------|--------------|
| Селивёрстов Дмитрий| RNN, отчёт                  |              |
| Меджидли Исмаил    | LSTM, отчёт                 |              |
| Дубровин Дмитрий   | Двунаправленная LSTM, отчёт |              |

## RNN

Для обучения RNN была выбрана книга «Гарри Поттер», которая была конвертирована из HTML-файла в текст. Для модели были использованы два типа токенизации: на уровне символов и на уровне слов. Модель прошла 20 эпох для посимвольной токенизации и 30 эпох для токенизации на уровне слов. Каждое последующее слово/символ выбиралось случайно с учетом вероятностей, предсказанных моделью.

### Посимвольная модель
Модель обучалась на последовательностях длиной 40 символов с шагом 3. Для обучения использовались две слои RNN по 128 нейронов с добавлением слоев Dropout для предотвращения переобучения. Пример сгенерированного текста из 100 символов на основе строки "Однажды":
```
Однажды...  …   .,  ,    .»,: ?...     .,., . .., . .   ... . ,.   .  . , . .., ,..,,. .  ..,,  .,. ..    .
```
Сгенерированный текст практически не имеет смысла, что связано с тем, что модель обучена на уровне символов. Она может предсказать отдельные символы, но не всегда формирует осмысленные слова или предложения.

### Модель на уровне слов
Для модели с токенизацией по словам было использовано до 10 000 уникальных слов, каждая последовательность имела длину 40 слов. Пример сгенерированного текста на основе фразы "Это начало текста для генерации":
```
Это начало текста для генерации в в в в в в в в в в в в в в в в в в в в в в в в в в не в занятий директора — я не могу рассказать тебе — сказал гарри — я не
```
Текст имеет больше осмысленных слов, но его структура страдает от повторений, что может быть связано с ограниченным количеством данных для обучения.

Модели RNN продемонстрировали возможность генерации текста на основе как символов, так и слов. Однако качество генерируемого текста пока остается на низком уровне, особенно для посимвольной модели. Из-за ограничений вычислительных ресурсов и объема данных обучение на большом корпусе текстов представляется затруднительным. Например, обучение посимвольной модели на протяжении 20 эпох заняло 1 час, а обучение пословной модели — около 3 часов.

## LSTM

## Двунаправленная LSTM

## Вывод
